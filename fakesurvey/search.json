[
  {
    "objectID": "source3.html",
    "href": "source3.html",
    "title": "Nonresponse Bias Analysis",
    "section": "",
    "text": "Declining response rates all over the world increase the fear of nonresponse bias, i.e., that the respondents to a survey do not well represent the group of individuals who has been invited to participate in that survey. In the presence of nonresponse bias, raw survey estimates can not be used to draw valid conclusions on the population of interest.\nHigh nonresponse does not necessarily imply high nonresponse bias. This guideline discusses the relationship between nonresponse and nonresponse bias and gives an overview of methods to determine nonresponse bias for a specific survey or survey variable of interest. Talking about survey nonresponse, we can in general distinguish between item nonresponse and unit nonresponse. Unit nonresponse means that an individual who is sampled and invited to participate in a survey does not participate in that survey at all. Item nonresponse occurs if an interviewed person does not give an answer to a specific question. This guideline captures unit nonresponse, for a discussion of the handling of item nonresponse we refer to the GESIS survey guideline on imputation (Bruch, 2023). Adjustment methods might be applied to reduce nonresponse bias but they only work under certain conditions that are discussed in this guideline. This guideline does, however, not address the treatment nor the prevention of nonresponse bias. For the former, we recommend the survey guidelines on weighting (Gabler et al., 2015; Sand & Kunz, 2020), for the latter the survey guideline on nonresponse bias (Koch & Blohm, 2015).\nNonresponse is by far not the only source of potential survey error (Groves & Lyberg, 2010). For simplicity, this guideline ignores all other sources of error, for example, we assume simple random sampling and measurements without error or item nonresponse. The next section discusses the relationship between survey nonresponse and nonresponse bias. Several univariate and multivariate indicators for the risk of nonresponse bias are discussed in Section 3. In Section 4, we illustrate some of the nonresponse bias indicators using a synthetic data example. A R-file to replicate the example is made available online. We conclude with a discussion in Section 5."
  },
  {
    "objectID": "source3.html#threemodels",
    "href": "source3.html#threemodels",
    "title": "Nonresponse Bias Analysis",
    "section": "2.1 Nonresponse mechanisms",
    "text": "2.1 Nonresponse mechanisms\nGroves (2006) distinguishes between three nonresponse mechanisms that can be explained by different models:\nSeparate Cause Model: The response propensity \\(\\rho\\) depends on personal characteristics \\(Z\\) that are not associated with the variable of interest \\(Y\\). \\(Y\\) is associated with personal characteristics \\(X\\) that do not affect the response propensity \\(\\rho\\). In this situation, \\(Y\\) and \\(\\rho\\) are not associated. This means that nonresponse does not lead to nonresponse bias in the analysis of \\(Y\\).\nAs Groves (2006) notes, completely unrelated causes are hard to imagine in practice. The separate cause model is, however, very useful when thinking of the relationship of nonresponse and nonresponse bias and to contrast the other models against it.\nCommon Cause Model: The same individual characteristics \\(Z\\) affect the response propensity \\(\\rho\\) and the variable of interest \\(Y\\). The common cause generates an association between \\(\\rho\\) and \\(Y\\) thus potentially biasing the analysis of \\(Y\\).\nIf \\(Z\\) is known for respondents and nonresponents, it can be used to perform nonresponse adjustments in the analysis of the survey variable \\(Y\\) and to reduce nonresponse bias. Like the separate cause model, the common cause model is a simplified model. In practical applications, there will most likely be unboserved \\(Z\\)-variables that can not be included in the nonresponse adjustment.\nSurvey Variable Cause Model: The variable of interest \\(Y\\) directly affects the response propensity \\(\\rho\\). Since \\(\\rho\\) and \\(Y\\) are associated, analysis of \\(Y\\) will suffer from nonresponse bias, and this can not be completely removed by any weighting or adjustment method.\nIt is important to note that different nonresponse models might hold for different variables \\(Y\\) of the same survey. The nonresponse mechanism and thus nonresponse bias is always variable-specific."
  },
  {
    "objectID": "source3.html#different-perspectives-on-nonresponse-bias",
    "href": "source3.html#different-perspectives-on-nonresponse-bias",
    "title": "Nonresponse Bias Analysis",
    "section": "2.2 Different perspectives on nonresponse bias",
    "text": "2.2 Different perspectives on nonresponse bias\nNonresponse bias can be viewed from several perspectives that highlight different facets.\nWe differentiate between three groups: The overall target population (with size \\(N^*\\)), the sampled individuals (with size \\(N\\)), and the survey respondents (with size \\(n\\)), where \\(N^* &gt; N \\geq n\\). In this guideline, we assume that the survey sample is randomly drawn from the target population. For ease of exposition we further assume a simple random sample, i.e., sampling with equal inclusion probabilities.\nWith \\(y_i\\) being the value for survey variable \\(Y\\) for individual \\(i\\), the population mean of \\(Y\\) is given by \\(\\bar{y}_P = 1/N^* \\sum_{i=1}^{N^*} y_i\\), the mean of the sampled individuals by \\(\\bar{y}_S = 1/N \\sum_{i=1}^{N} y_i\\), and the mean of the survey respondents by \\(\\bar{y}_R = 1/n \\sum_{i=1}^{n} y_i\\). For more complex survey designs with unequal inclusion probabilities, survey estimates must to be design-weighted.\nNonresponse bias (NRB) in the estimated mean of a survey variable \\(Y\\) is given by the difference between the mean value of the survey respondents \\(\\bar{y}_R\\) and the mean of the target population \\(\\bar{y}_P\\). Assuming random sampling, \\(\\bar{y}_P = \\bar{y}_S\\) holds, such that\n\\[\n\\begin{align}\nNRB_Y = \\bar{y}_R - \\bar{y}_S.\n\\end{align}\n\\]\nAs can easily be seen, we do not have to expect nonresponse bias in \\(\\bar{y}_R\\) if respondents do not differ from the target population in \\(Y\\) on average. Nonresponse bias gets larger as the difference increases.\nLooking at this relationship more closely, usually the deterministic and the stochastic view on nonresponse bias are distinguished. Even though they refer to the exact same concept, they highlight different aspects making it worth to look at both of them.\nThe deterministic view on nonresponse bias is given by (see for example Groves, 2006):\n\\[\n\\begin{align}\nNRB_Y = (1-RR) (\\bar{y}_R - \\bar{y}_{NR}),\n\\end{align}\n\\]\nwhere \\(RR = \\frac{n}{N}\\) is the response rate and the mean of the nonrespondents is given by \\(\\bar{y}_{NR}\\). Nonresponse bias is affected by the response rate and the difference between means for respondents and nonrespondents. This means two things: For a given difference between respondents and nonrespondents, an increasing response rate will lower nonresponse bias. For a given response rate, lower differences between respondents and nonrespondents lead to lower nonresponse bias. If respondents and nonrespondents do not differ in \\(Y\\) at all, no nonresponse bias is to be expected in \\(Y\\).\nThe stochastic approach takes the perspective that participants are not determined to be either respondents or nonrespondents, but characterized by a latent, stochastic propensity to respond. Taking this approach, nonresponse bias is computed over all elements of the target population, weighted by their unobserved response propensities \\(\\rho_i\\). This gives rise to the following definition (Bethlehem, 1988):\n\\[\n\\begin{align}\nNRB_{Y} &  \\approx \\frac{1}{\\bar{\\rho}} Cov(y, \\rho)\n\\nonumber \\\\\n& \\approx \\frac{1}{\\bar{\\rho}} Cor (y, \\rho) \\sigma_y \\sigma_\\rho\n\\end{align}\n\\]\nwhere \\(\\rho\\) is the vector (of length \\(N^*\\)) of response propensities with population mean \\(\\bar{\\rho}\\). The population covariance of \\(y\\) and \\(\\rho\\) is given by \\(Cov(y, \\rho) = 1/N^* \\sum_{i=1}^{N^*} (y_i - \\bar{y}_P)(\\rho_i - \\bar{\\rho})\\). The population standard deviations of \\(y\\) and \\(\\rho\\) are \\(\\sigma_y = \\sqrt{{1}/{N^*} \\sum_{i=1}^{N} (y_i - \\bar{y})^2}\\) and \\(\\sigma_\\rho = \\sqrt{{1}/{N^*} \\sum_{i=1}^{N} (\\rho_i - \\bar{\\rho})^2}\\), and \\(Cor(y, \\rho) = \\frac{Cov (y, \\rho)}{\\sigma_y \\sigma_\\rho}\\) is the population correlation of \\(y\\) and \\(\\rho\\).\nAs the stochastic view highlights, nonresponse bias decreases with increasing \\(\\bar{\\rho}\\) (which corresponds to the response rate), decreasing correlation of \\(Y\\) and \\(\\rho\\), and decreasing standard deviations of both \\(\\rho\\) and \\(y\\). If \\(Cor(y, \\rho) = 0\\), i.e., if the nonresponse mechanism is not related to \\(Y\\) at all, no nonresponse bias is to be expected. The same is true if \\(\\sigma_{\\rho}= 0\\) (all individuals have the same propensity to respond) or \\(\\sigma_y = 0\\) (all individuals have the same value of \\(Y\\))."
  },
  {
    "objectID": "source3.html#why-is-the-response-rate-alone-not-a-reliable-indicator-of-nonresponse-bias",
    "href": "source3.html#why-is-the-response-rate-alone-not-a-reliable-indicator-of-nonresponse-bias",
    "title": "Nonresponse Bias Analysis",
    "section": "2.3 Why is the response rate alone not a reliable indicator of nonresponse bias?",
    "text": "2.3 Why is the response rate alone not a reliable indicator of nonresponse bias?\nSurveys often report the response rate as an indicator for the quality of the survey. As can be seen from the formulas above, the response rate is part of the deterministic and stochastic perspective on nonresponse bias. Keeping the other factors constant, nonresponse bias is lower the higher the response rate is. There is, however, no clear relationship between the response rate of a survey and the other factors that constitute nonresponse bias. The response rate alone does not allow for an evaluation of nonresponse bias and is thus not a good nonresponse bias indicator (for empirical findings see for example Groves, 2006; Groves & Peytcheva, 2008)."
  },
  {
    "objectID": "source3.html#components-of-nonresponse-bias-analysis",
    "href": "source3.html#components-of-nonresponse-bias-analysis",
    "title": "Nonresponse Bias Analysis",
    "section": "3.1 Components of nonresponse bias analysis",
    "text": "3.1 Components of nonresponse bias analysis\nMany of the parameters discussed in Equations (1) to (3) are not known but can be estimated based on the survey information. The mean of the respondents can be estimated by the survey mean \\(\\hat{\\bar{y}} = \\frac{1}{n} \\sum_{i=1}^n y_i\\) where \\(y_i\\) (\\(i = 1, \\ldots, n\\)) is the survey value for the \\(i^{th}\\) respondent. The response propensity of the invited individuals \\(\\rho\\) is not known. In many cases, auxiliary variables \\(X\\) are available for respondents and nonrespondents that can be used to estimate the response propensity. They might be available from the sample frame (e.g., age and gender from official registers), administrative data, paradata from the sampling or recruitment process or, in the panel context, be survey answers from previous survey waves. To estimate the response propensity \\(\\rho\\), the participation indicator R (R=1 if the individual responds to the survey and zero otherwise) is regressed on multiple (\\(v\\)) \\(X\\)-variables, commonly using logistic regression such as\n\\[\n\\begin{align}\n\\hat{\\rho_i}= P(R_i=1) = \\frac{exp(\\hat{\\beta}_0 + \\hat{\\beta}_1\nx_{i1}+ \\ldots + \\hat{\\beta}_p x_{iv}) }{1 + exp(\\hat{\\beta}_0 + \\hat{\\beta}_1\nx_{i1} + \\ldots + \\hat{\\beta}_p x_{iv})}\n\\end{align}\n\\]\nwhere \\(\\hat{\\rho}_i\\) is the estimated response propensity for the \\(i^{th}\\) individual, \\(i = 1 \\ldots N\\) (the full sample), \\(\\hat{\\beta}_0\\) is the intercept and \\(\\hat{\\beta}_1 \\ldots \\hat{\\beta}_v\\) are the slopes for the observed auxiliary variables \\(X_1 \\ldots X_v\\); \\(x_{i1} \\ldots x_{iv}\\) are the values of the \\(X\\)-variables of individual \\(i\\). For large data sets, machine learning methods might be preferred over standard logistic regression, see for example Felderer et al. (2023).\nThe population parameter \\(\\bar{y}_p\\) is usually not known – that is why we conduct the survey in the first place – and can not be estimated from the survey. The same is true for the parameters \\(\\bar{y}_S\\) and \\(\\bar{y}_{NR}\\).\nThere are several indicators available to evaluate the risk of nonresponse bias that can be roughly put into two categories: Indicators that refer to the risk of nonresponse bias of a whole survey and indicators that focus on specific survey variables. For the latter, one can distinguish indicators that basically refer to auxiliary variables and indicators that refer to the variable of interest.\nNonresponse bias as introduced above can usually only be estimated for \\(X\\)-variables that are known for respondents and nonrespondents or for which population benchmarks are available. The \\(Y\\)-variable is usually unobserved for the nonrespondents and lacks a population benchmark. We thus can not study nonresponse bias in the \\(Y\\)-variable directly but rather the risk of nonresponse bias that we derive from knowledge about nonresponse bias in the \\(X\\)-variables and the relation between \\(X\\) and \\(Y\\).\nMany indicators that we introduce in the following section are generated to apporach nonresponse bias in different ways refering to single aspects of equation (3). They consequently do not estimate nonresponse bias in the strict sense but rather the risk of nonresponse bias (in \\(X\\) or in \\(Y\\))."
  },
  {
    "objectID": "source3.html#multivariate-nonresponse-bias-indicators",
    "href": "source3.html#multivariate-nonresponse-bias-indicators",
    "title": "Nonresponse Bias Analysis",
    "section": "3.2 Multivariate nonresponse bias indicators",
    "text": "3.2 Multivariate nonresponse bias indicators\nSeveral measures for the risk of nonresponse bias have been developed that are based on auxiliary information on respondents and nonrespondents. At their essence, these measures attempt to estimate the extent to which individuals in the survey resemble those in the gross sample or population with respect to the auxiliary \\(X\\)- variables. These results are then used to infer possible bias in the \\(Y\\)- variables of interest. The usefulness of the indicators to evaluate the risk of nonresponse bias in a specific variable of interest heavily depends on the association of this variable and the auxiliary variables. If both variables are not related at all, nonresponse bias in \\(X\\) is not a good indicator for nonresponse bias in \\(Y\\). The stronger the variables are related, the more we expect \\(Y\\) to show nonresponse bias if \\(X\\) does. The following indicators are multivariate in a way that they account for several auxiliary variables and their relationships in preducing survey nonresponse. Two of them allow to analyse the effect of nonresponse on specific survey variables of interest. The interpretation of the multivariate nonresponse bias indicators is limited to the specific auxiliary variables that they are built on. An excellent comparison of indicators for the risk of nonresponse bias can be found in Wagner (2012).\n\nR-indicator\nThe R-indicator \\(R(\\rho)\\) (Schouten et al., 2009) takes the variation of the response propensity \\(\\rho\\) as a measure for the risk of nonresponse bias. With \\(\\sigma_\\rho\\) being the population standard deviation of \\({\\rho}\\), the R-indicator is given by:\n\\[\n\\begin{equation}\nR(\\rho) = 1 - 2 \\sigma_\\rho\n\\end{equation}\n\\]\nThe R-indicator can take values between 0 and 1. If all sampled individuals have the same propensity to respond, the standard deviation of the response propensities \\(\\sigma_\\rho\\) is 0 and the R-indicator takes the value of 1. This means that the survey is perfectly “representative”. The R- indicator is zero if \\(\\sigma_\\rho= 0.5\\) which is the highest value \\(\\sigma_\\rho\\) can take. This means that the response propensities are very different and the risk of nonresponse bias is high. Only if the auxiliary variables are correlated with both \\(\\rho\\) and the variable of interest, the R-indicator can give a good impression on the risk of nonresponse bias in the variable of interest.\nIn practical applications, the R-indicator can be estimated by \\(\\hat{R}(\\hat{\\rho}) = 1 - 2 \\hat{\\sigma}_{\\hat{\\rho}}\\) where \\(\\hat{\\rho_i}\\) are the estimated response propensities based on auxiliary information as described in Equation (4). Its standard deviation is estimated by \\(\\hat{\\sigma}_{\\hat{\\rho}} = \\frac{1}{N} \\sum_{i=1}^n (\\hat{\\rho}_i - \\hat{\\bar{\\rho}})^2\\) where \\(\\hat{\\rho}_i\\) is the estimated response propensity for the \\(i^{th}\\) individual and \\(\\hat{\\bar{\\rho}}\\) is the mean of the estimated propensities.\nThe R-indicator gives an impression whether the different population subgroups, characterized by a combination of \\(X\\)-variables, are well represented in the survey. The R-indicator does not allow to determine how the individual auxiliary variables that are used to estimate the R-indicator contribute to representativeness. Partial R-indicators have been developed to overcome this limitation (Schouten et al., 2010).\nAs for all multivariate nonresponse bias indicators, the usefulness of the R-indicator depends on the auxiliary variables that are used to estimate \\(\\hat{\\rho}\\). The R-indicator can only be interpreted with regard to the auxiliary information that it builds on (see for example Roberts et al., 2020). R-Indicators are frequently used to compare different (sub)-samples. Comparisons of R-Indicators are only meaningful if the R-indicators are build in the exact same way including the same auxiliary variables. Higher values of the R-indicators indicate better representativeness. However, there is no agreement in the literature on the threshold value above which one can speak of good representativeness. As a guide, an R-indicator of \\(0.7\\) is considered to be rather low (Lugtig et al., 2022).\n\n\nGoodness of fit of the propensity model\nThe propensity model for \\(\\hat{\\rho_i}\\) (see Equation (4)) can be further analysed. The coefficients of the \\(X\\)-variables allow for an interpretation of which variables influence the response propensity. If the response propensity does not depend on \\(X\\)-variables, we expect all coefficients to be insignificant and close to zero. Measures for the goodness of fit of the propensity model, like the (pseudo) \\(R^2\\) or the area under the curve (AUC) are taken as indicators for the risk of nonresponse bias (see for example Groves et al., 2008). Higher values in these measures means that more variation in \\(\\hat{\\rho}\\) can be explained by the \\(X\\)-variables. This means that respondents and nonrespondents differ for their values of \\(X\\). A higher goodness of fit of the propensity model thus is taken as an indicator for a higher risk of nonresponse bias in estimates of the variable of interest.\nLike the R-indicator, the goodness of fit of the propensity model, however, is only a good indicator for the risk of nonresponse bias in the variable of interest if this variable is associated with \\(X\\). Finding that the observed \\(X\\)-variables that are included in the propensity model do not explain variation in \\(\\hat{\\rho}\\) does, on the other side, not necessarily mean that the risk of nonresponse bias is low as there may exist unobserved variables that systematically affect nonresponse.\n\n\nVariation of nonresponse weights\nPropensity models like in Equation (4) can be used to create adjustment/nonresponse weights by taking the inverse of the estimated response propensity (\\(1/ \\hat{\\rho}_i\\)) (Little, 1986). The rough idea behind adjustment methods is to give the groups of respondents who have a lower propensity to respond to the survey (given \\(X\\)-variables) a higher weight in the analysis of the survey data.\nThe variance of nonresponse weights can be taken as an indicator for the risk of nonresponse bias (Groves et al., 2008): If all individuals have the same propensity to respond, the variance of the nonresponse weights is zero. The higher the variance is, the larger are the differences in the response propensities and the higher the risk of nonresponse bias. This sort of analysis can be conducted for other kinds of nonresponse weighting methods as well and is not limited to propensity weights. Of course, the limitations concerning the relations between \\(X\\)-variables and \\(Y\\) that we discussed above also hold for this and other multivariate nonresponse bias indicators.\n\n\nCorrelation between nonresponse weights and \\(Y\\)\nThe correlation between the nonresponse weights and observed \\(Y\\) is an attempt to estimate the association of the auxiliary variables and the response propensity and \\(Y\\) (Groves et al., 2008). The association can only be estimated for respondents and relies on the assumption that the association is the same for respondents and the full sample. A higher correlation between nonresponse weights and \\(Y\\) indicates a higher risk of nonresponse bias. In this case, we will also observe differences between weighted and unweighted means and proportions of \\(Y\\) (see Gabler et al. (2015) and Sand & Kunz (2020) on the theoretical background and application on different kinds of nonresponse weights). The correlation between nonresponse weights and \\(Y\\) is an indicator for nonresponse bias before weighting adjustment. It does not allow any conclusions about nonresponse bias of the adjusted statistics or the usefulness of the weights. The correlation between nonresponse weights and \\(Y\\) is only a meaningful indicator for nonresponse bias under the MAR-assumption.\n\n\nFraction of missing information\nThe fraction of missing information (FMI) was developed in the multiple imputation context (see Rubin, 1987) dealing with item nonresponse and has been transferred to unit nonresponse bias analysis (Wagner, 2010). In the context of unit nonresponse, the missing survey information of nonrespondents is imputed using auxiliary data that is available for respondents and nonrespondents. The idea is to take the level of uncertainty when imputing the missing values for the variable of interest \\(Y\\) based on auxiliary (complete) \\(X\\) as an indicator for nonresponse bias. For the FMI, the missing observations in the \\(Y\\)-variables are imputed multiple (\\(M\\)) times. The mean or proportion of \\(Y\\) is then estimated based on the fully imputed data set (including observed values for respondents and imputed values for nonrespondents) for each imputation round \\(m\\).\nWith \\(M\\) imputations for each missing value, \\(\\bar{y}\\) is estimated by \\(\\hat{\\bar{y}}_M = \\sum_{m=1}^M \\hat{\\bar{y}}_m /M\\) where \\(\\hat{\\bar{y}}_m\\) is the estimated mean in the \\(m_{th}\\) imputed data set. The FMI gives a measure of uncertainty about the imputed values by computing the relation between the between-imputation variance and the total variance of \\(\\hat{\\bar{y}}_M\\). Let \\({Var}_m(\\hat{\\bar{y}}_m)\\) be the variance of \\(\\hat{\\bar{y}}_m\\) in the \\(m_{th}\\) imputed data set. The within-imputation variance is given by \\(Var_W(\\hat{\\bar{y}}_M)= \\sum_{m=1}^M Var_m(\\hat{\\bar{y}}_m)/M\\). The within-imputation variance describes the variance that is due to sampling. The between-imputation variance is given by \\(Var_B(\\hat{\\bar{y}}_M) = \\frac{\\sum_{m=1}^M (\\hat{\\bar{y}}_m - \\hat{\\bar{y}}_M)^2}{(M-1)}\\) and is the part of variation that is due to imputation uncertainty. The total variance is given by \\(Var(\\hat{\\bar{y}}_M) = Var_W(\\hat{\\bar{y}}_M) + (M+1) M^{-1} Var_B(\\hat{\\bar{y}}_M)\\).\nThe FMI is given as the ratio of the between-imputation and total variance and is estimated as\n\\[\n\\begin{equation}\n\\widehat{FMI} = \\frac{(1+\\frac{1}{M})\nVar_B(\\hat{\\bar{y}}_M)}{Var(\\hat{\\bar{y}}_M)}\n\\end{equation}\n\\]\nThe FMI ranges from 0 to 1 and can be interpreted as the proportion of variation in the estimation of \\(\\bar{y}\\) that is due to the missing data. The higher the uncertainty about the values of \\(Y\\) given \\(X\\), the more different are the imputed values between the imputation rounds and the higher is the between-imputation variance. Or, in other words, if the data includes good predictors for \\(Y\\), the between-imputation variance decreases (Rubin (1987)). A larger \\(\\hat{FMI}\\) is thus interpreted to indicate a higher risk of nonresponse bias in the estimation of \\(\\hat{\\bar{y}}\\) caused by other variables than included in the imputation process.\nIf the imputation model perfectly explains \\(Y\\), there are no differences expected between the imputation rounds and the between-imputation variance is close to zero. As Wagner (2010) shows, for correctly specified imputation models, the FMI is close to the nonresponse rate if \\(Y\\) and \\(X\\) are only weakly correlated and moves toward zero as the correlation increases. Highly correlated \\(X\\) variables can thus recover the missing information in \\(Y\\).\nAs Andridge & Little (2011) note the FMI has the disadvantage to focus more on precision than on bias and is limited to MAR situations. The FMI also depends on the imputation methods and the specification of the imputation model (Wagner, 2010)."
  },
  {
    "objectID": "source3.html#univariate-nonresponse-bias-indicators",
    "href": "source3.html#univariate-nonresponse-bias-indicators",
    "title": "Nonresponse Bias Analysis",
    "section": "3.3 Univariate nonresponse bias indicators",
    "text": "3.3 Univariate nonresponse bias indicators\nThe multivariate nonresponse bias indicators introduced in the previous section are measures including several \\(X\\)-variables and their relationships. With the exception of the interpretation of the coefficients of the nonresponse propensity model, they do not allow for an interpretation which specific \\(X\\) variables contribute to bias and in which direction. Although it is possible to run nonresponse models with just a single variable and use this model to estimate the nonresponse bias indicators discussed in the previous section, other methods are better suited to look at the effects of individual variables. Such univariate nonresponse bias indicators give more detailed information on nonresponse bias of certain variables and can be used to evaluate which of the auxiliary variables affect the risk of nonresponse bias in the variables of interest. This is done by comparing respondents to official population benchmarks or respondents to nonrespondents for these auxiliary variables. Univariate nonresponse bias indicators do not account for the interplay of different auxiliary variables. Like the nonresponse bias indicators discussed in the previous section, univariate nonresponse bias indicators are only useful under the assumption that the auxiliary variables are related to the variable of interest.\n\nBenchmark comparisons\nOne way to evaluate nonresponse bias is to compare survey findings to benchmarks from official statistics for the general population (see for example Felderer et al., 2019; Rohr et al., 2023). For surveys on the German population, the distribution of socio-demographic characteristics among the respondents to a survey might, for example, be compared to official statistics like the  or to a high-quality survey of the German population like the . If the survey respondents are similar to the general population, the risk of nonresponse bias is assumed to be low. Benchmark comparisons thereby implicitly rely on the assumption that the survey estimate \\(\\hat{\\bar{x}}\\) is not subject to bias apart from nonresponse. Other sources of bias like sampling or coverage error are neglected. The advantage of benchmark comparisons is that no information on nonrespondents are needed. However, the number of variables that can be compared against benchmarks is usually rather limited. Usually, no benchmarks are available for the variables of interest \\(Y\\).\nLet \\(\\hat{\\bar{x}}\\) be the survey mean and \\(\\bar{x}_{bench}\\) the population benchmark for the auxiliary variables. To be able to compare nonresponse bias over variables and between surveys, the relative nonresponse bias for a variable \\(x_j\\) is estimated as\n\\[\n\\begin{equation}\n\\widehat{rel. Bias}(\\bar{x}_j) = \\frac{\\hat{\\bar{x_j}} -\n\\bar{x_j}_{bench}}{\\bar{x_j}_{bench}}.\n\\end{equation}\n\\]\nIn practice, the differences between \\(\\hat{\\bar{x}}\\) and \\(\\bar{x}_{bench}\\) will be non-zero due to random sampling variation. Appropriate statistical tests can be performed to evaluate statistical significance of the differences (see for example Eckman et al., 2022; Felderer et al., 2019).\nIn practical applications, the relative biases are often estimated for a number of available auxiliary variables. All relative biases might be aggregated to one single measure that can be compared across surveys or experimental subgroups of a survey. A commonly reported measure is the average absolute relative bias (AARB) (see for example Cornesse et al., 2021; Friedel et al., 2023) which is given by the mean of the absolute relative biases. The absolute values are taken to make sure that negative and positive relative biases do not cancel each other out. For \\(v\\) auxiliary variables the AARB is given by\n\\[\n\\begin{equation}\n\\widehat{AARB} = \\frac{1}{v} \\sum_{j=1}^v \\left|\\frac{\\bar{x}_{j} -\n\\bar{x}_{bench_j}}{\\bar{x}_{bench_j}}\\right|\n\\end{equation}\n\\]\nwith subscripts \\(j = 1 \\ldots v\\) indicating the \\(j^{th}\\) \\(X\\)-variable. Relative biases can also be aggregated to median absolute relative bias and the maximum absolute relative bias. These aggregated measures have the advantage to reduce the findings for several \\(X\\)-variables to one single value but they are not a multivariate indicator in our sense as they do not account for the interplay of different \\(X\\)-variables. As they do not allow to identify the contribution of the individual characteristics, we recommend to not only analyse the aggregate measures but also their single components.\nThe AARB is not standardized and its interpretation is only meaningful in comparison to relative biases found in other surveys or for experimental subgroups of the same survey. In order to meaningfully compare AARBs between surveys, one needs to make sure that the same set of auxiliary variables (that are measured and coded in the same way) are included in the analysis. High nonresponse bias in the auxiliary variables might indicate a high risk of nonresponse bias in the variables of interest. If both kinds of variables are not correlated, even high nonresponse bias in auxiliary variables is no indication of nonresponse bias in the survey variables of interest.\nOther measures that are based on benchmark comparisons are the Duncan dissimilarity index (for example Bosnjak et al., 2018) and absolute difference or standardized absolute difference (for example Peytcheva & Groves, 2009).\n\n\nComparison of respondents and nonrespondents on auxiliary variables\nUsually,the number of survey variables that can be compared to official statistics is very limited. In many applications, however, auxiliary variables \\(X\\) from the sample frame, paradata from the fieldwork process (for example Krueger & West, 2014) or, in a panel context, survey information from previous waves are available for all individuals who are invited to participate in a survey. Comparisons between respondents and nonrespondents can be performed by comparing means and proportions and determining significance using appropriate statistical tests.\n\n\nComparison of early and late respondents\nA comparison of early or “easy-to-contact” respondents to late or “hard-to-contact” respondents is sometimes used to get an impression of the risk of nonresponse bias (see for example Green, 1991). Doing this, the late respondents are assumed to be similar to nonrespondents. While this approach relies on strong assumptions, it has the advantage that it can be performed on auxiliary variables as well as the survey variables of interest. To receive information on nonrespondents, sometimes a nonresponse follow-up survey (see for example Roberts et al., 2020) is conducted using a shortened questionnaire. The respondents to the main survey are then compared to the respondents of the nonresponse follow-up survey assuming that the latter are representative of all nonrespondents to the main survey.\n\n\nVariation of subgroup response rates\nThe evaluation of the variation of subgroup response rates is a univariate indicator that follows the same idea as the (multivariate) evaluation of the goodness of fit and nonresponse weights (see for example Wagner, 2012). If different subgroups (defined by the categories of a specific auxiliary variable) show different response rates, this is taken as an indication for an increased risk of nonresponse bias in the survey variable of interest. For a categorical variable with \\(c\\) categories the subgroup response rates are given as \\(RR_{sub, c} = \\frac{n_c}{N_c}\\) where \\(n_c\\) is the number of respondents in category \\(c\\) and \\(N_c\\) the number of sampled persons in category \\(c\\). \\(RR_{sub}\\) is the vector of the subgroup response rates for all categories of a specific variables. The variance of subgroup response rates can be estimated by:\n\\[\n\\begin{equation}\n\\hat{Var}(RR_{sub}) = \\frac{1}{N-1} \\sum_{c=1}^C N_c\n(\\frac{n_c}{N_c} - \\frac{n}{N})^2\n\\end{equation}\n\\]\nwhere \\(\\frac{n}{N}\\) is the survey’s response rate. The subgroup response rate does not require information on nonrespondents on an individual level but only sample (or population) proportions of the auxiliary variable. To standardize subgroup response rates, the coefficient of variation of the subgroup response rates (see for example Nishimura et al., 2016) can be calculated as\n\\[\n\\begin{equation}\n\\hat{CV}(RR_{sub}) =  \\frac{\\hat{Var}(RR_{sub})}{n/N}.\n\\end{equation}\n\\]\nHigher values of \\(\\hat{CV}(RR_{sub})\\) are taken as a higher risk of nonresponse bias. \\(\\hat{Var}(RR_{sub})\\) is only a useful indicator if \\(X\\) is associated with \\(Y\\)."
  },
  {
    "objectID": "source3.html#multivariate-nonresponse-bias-indicators-1",
    "href": "source3.html#multivariate-nonresponse-bias-indicators-1",
    "title": "Nonresponse Bias Analysis",
    "section": "4.1 Multivariate nonresponse bias indicators",
    "text": "4.1 Multivariate nonresponse bias indicators\nLet us consider two scenarios. In our first scenario, we have information on all five socio-demographic variables in our studies from the sample frame and use these information to conduct nonresponse bias analysis. In our second and more realistic scenario, some variables that affect nonresponse are not available from the frame and we only have a subset of the socio-demographic variables, gender, age and German citizenship, limiting our analysis to these variables.\nTo estimate multivariate nonresponse bias indicators, we run a logistic regression of the response indicator (yes/no) on all available frame information in each scenario. We estimate the R-indicator using the predicted probabilities from the logistic regression and build nonresponse weights as the inverse of the predicted probabilities. Table \\(\\ref{tab1}\\) shows the R-indicator, McFadden’s pseudo \\(R^2\\) from the logistic regression and the variance of the nonresponse weights.\nThe measures in the first three columns make use of all variables that are part of the nonresponse process for the separate cause model and the common cause model. The limited variable set used in the second scenario does not include all relevant variables. None of the variables used in any scenario affect the nonresponse process for the survey variable cause model.\nThe indicators need to be interpreted with caution and we must be aware which conclusions can be drawn and which not. All these measures only indicate to what extent the distributions of the \\(X\\)-variables in the survey correspond to those in the population. For the full variable set scenario, all indicators show very low risk of nonrespnse bias in the survey variable cause model but high risk for the other two models. That is expected, as we know that the data was generated that way. We should, however, not naively conclude that we have a generally low risk of nonresponse bias in this survey. Knowledge of the relationship of \\(X\\) and \\(Y\\), e.g., empirical evidence from other studies or theoretical considerations can help to judge whether to expect nonresponse bias in \\(Y\\). If we, for example, know from other studies that the \\(Y\\) variable is highly correlated to the \\(X\\)- variables (like it is the case for \\(Y_2\\)), we would assume a high risk of nonresponse bias in this variable of interest but not for a variable that is not related to \\(X\\), like \\(Y_1\\) in the same survey. In our example, \\(Y_3\\) is generated to not depend on the \\(X\\)-variables but to \\(Y_3\\) itself. Thus, the indicators are not meaningful when it comes to the risk of nonresponse bias in \\(Y_3\\). Likewise, there can always exist unknown or unobserved auxiliary variables that are related to \\(Y\\) and cause nonresponse bias. If these variables are not related to the \\(X\\)-variables that might capture parts of their effect, the high risk of nonresponse bias can not be detected. This illustrates the limitations of these indicators: they summarize the representation of different population subgroups (based on the \\(X\\)-variables) but do not allow to rule out effects of variables that are not part of the analysis, either because they can not be compared to some benchmark or are not observed at all.\nComparing the full variable set scenario to the limited variable set scenario, we find all indicators to “improve”: the R-indicators are higher whereas McFadden’s \\(R^2\\) and the variation of the nonresponse weights are closer to zero. These findings show how the specification of the nonresponse model that is part of these three measures influences the results. Interestingly, the mis-specified models in the limited variable set scenario that excludes relevant auxiliary variables misleadingly indicates a lower risk of nonresponse bias. This makes sense as leaving out relevant predictors in the nonresponse model decreases the model fit and thus McFadden’s \\(R^2\\) and decreases the variation of the predicted values for the nonresponse propensity. Again, the indicators can only be interpreted in relation to the specific \\(X\\)-variables. We can never know for sure that there are no unobserved characteristics that are excluded from the estimation of the nonresponse bias indicator (like in the limited variable set scenario) that systematically affect survey nonresponse.\nBoth scenarios show that the survey variable cause model in our example exhibits higher representativeness (of the \\(X\\)-variables) than the other models. Finding that the indicators heavily depend on the model specification, it is important to note that the indicators should only be compared between surveys if they include the same set of \\(X\\)-variables. Also, they should only be interpreted with respect to the \\(X\\)-variables they make use of. In the full variable set we can draw conclusions about the surveys’ representativeness with regard to gender, age, education, German citizenship and household size whereas in the limited variable set our conclusions are limited to gender, age and German citizenship.\nTable \\(\\ref{tab4}\\) shows the correlation between nonresponse weights and \\(Y\\) and the FMI in estimating \\(\\hat{\\bar{y}}\\). We show the results for the full information and limited information scenario described above. To estimate the FMI, we impute \\(Y\\) using all \\(X\\)-variable values that are available in the respective scenario. We conduct multiple imputations (\\(m=10\\)) using predictive mean matching as implemented in the R package mice (Van Buuren & Groothuis-Oudshoorn, 2011). The results are shown in Table \\(\\ref{tab4}\\).\nComparing the separate cause model and common cause model, we find that the correlation of weights and \\(Y\\) correctly identifies a higher risk in nonresponse bias for \\(Y\\) in the common cause model for the full variable set scenario. It does, however, not detect the high nonresponse bias in \\(Y\\) in the survey variable cause model. This was expected as there is no relationship between the \\(X\\)-variables and \\(Y\\) for this model. Consequently, findings are very similar for the survey variable cause model and the separate cause model for which \\(X\\) and \\(Y\\) are also not associated. Whereas for the separate cause model we are right to conclude that there is only a low risk of nonresponse bias in \\(Y\\) we would be wrong in the survey variable cause model. For the limited variable set scenario, the correlations are estimated to be of about the same size for all of the three nonresponse models, failing to detect the high risk of nonresponse bias in the common cause model. Again, the nonresponse bias indicators can only be interpreted in relation to the \\(X\\)-variables they are built on and they are not able to detect a correlation of \\(Y\\) and \\(\\rho\\) or of \\(Y\\) and unobserved \\(X\\)-variables that are not part of the propensity model.\nThe FMI can be interpreted by comparing its value to the nonresponse rate. As Nishimura et al. (2016) point out, the FMI is bounded by the nonresponse rate under the MAR model. Observing values that are much higher than the nonresponse rate (50 % in our case) is an indication that the imputation model is mis-specified. If the imputation model is correctly specified, the FMI should not be larger than about \\(0.5\\). The higher the correlation of \\(Y\\) and \\(X\\), the stronger the FMI decreases towards zero. For the limited variable set scenario we find all FMIs to be larger than \\(0.5\\) indicating that the models are mis-specified and do not caputure all \\(X\\)-variables that relate to the response propensity. For the full variable set scenario, the FMI is larger than \\(0.5\\) for the separate cause model and survey variable cause model. This again can be explained by the fact that \\(X\\) is not correlated with \\(Y\\) and using \\(X\\) for the imputation for \\(Y\\) has no positive effect on the between-imputation-variance of \\(Y\\). As noted above, both models do not lead to data that are missing at random and thus do not meet the assumptions of the FMI. We are not able to distinguish between the separate cause and survey variable cause model based on the FMIs. Whereas for the survey variable cause model we are right to conclude that we observe a high risk of nonresponse bias the large FMI in the separate cause model is misleading. The FMI for the common cause model reflects the situation very well. It is lower than the nonresponse rate indicating correct model specification. Due to random error in the generation of \\(Y\\), \\(X\\) and \\(Y\\) are not perfectly correlated and the FMI is not exactly zero.\nThe multivariate nonresponse bias indicators are useful measures for the representativeness of a survey regarding the specific variables they are built on. They do, however, not allow for an evaluation of which variables are well represented and which are not. Most importantly, taking them as indicators for the risk of nonresponse bias in \\(Y\\) might be very misleading. The representativeness of \\(X\\) can only be used as an indicator for nonresponse bias in \\(Y\\) under the assumption that \\(X\\) and \\(Y\\) are highly correlated on the population level. This assumption usually cannot be tested. Only substantive knowledge on the relationships between \\(X\\) and \\(Y\\) can help to evaluate whether the assumption holds.\nSeveral indicators that allow for an evaluation of the risk of nonresponse bias on the variable level are illustrated in the following section."
  },
  {
    "objectID": "source3.html#univariate-nonresponse-bias-indicators-1",
    "href": "source3.html#univariate-nonresponse-bias-indicators-1",
    "title": "Nonresponse Bias Analysis",
    "section": "4.2 Univariate nonresponse bias indicators",
    "text": "4.2 Univariate nonresponse bias indicators\nLet us assume we know the means and proportions for the socio-demographics (\\(X\\)-variables) from official statistics such as the Mikrozensus. We can easily compare the survey estimates based on the respondents and the population values from the Mikrozensus for the \\(X\\)-variables for the three nonresponse models. We should be aware, however, that even for random sampling of the individuals, the survey estimates likely do not exactly match the population parameters by chance. Like in real applications, we are not able to separate random sampling error from nonresponse error.\nTo be able to compare the magnitude of nonresponse bias between \\(X\\)-variables and different surveys, we compute the relative biases using benchmarks from official statistics and the AARB (see Table \\(\\ref{tab3}\\)). For illustration, we assume that we also know the true distribution of \\(Y\\) which is usually unknow. This allows us to estimate relative nonresponse bias in \\(Y\\) as well. The relative biases in \\(X\\) are the same for the separate cause model and the common cause model but differ from the ones for the survey variable cause model.\nWe can, for example, see that the younger age cohorts are under- and the older age cohorts are over-represented for all surveys. The mis-representation is strongest for the individuals aged 16 to 29 years. The AARB gives a summary of the biases and naturally shows the same trend as the indicators from the previous section: the survey variable cause model shows the highest representativeness or, in other words, lowest AARB for the socio-demographic variables under study.\nIn this example, we are able to evaluate bias in the \\(Y\\)-variables as well. The separate cause model and common cause model examples are generated to be the same survey with different \\(Y\\)-variables of interest. We can see that the \\(Y\\)-variable in the separate cause model example is under-estimated to a very small degree due to random sampling error whereas the \\(Y\\)-variable from the common cause model shows an over-estimation of more than 5%. This shows that variables from the same survey can be affected by nonresponse very differently depending on how they are related to the nonresponse mechanism. In our example, nonresponse bias is highest for the \\(Y\\)-variable from the survey variable cause model. Even though the survey was very representative for all \\(X\\)-variables, \\(Y\\) suffers from severe nonresponse bias. This example shows that a good/bad representation of \\(X\\) does not necessarily imply a good/bad representation of \\(Y\\). This example made use of the population information of \\(Y\\) which is usually not available."
  },
  {
    "objectID": "source.html",
    "href": "source.html",
    "title": "Nonresponse Bias",
    "section": "",
    "text": "“No issue in survey research is more misunderstood or controversial than nonresponse.” (Dixon & Tucker, 2010)"
  },
  {
    "objectID": "source.html#comparison-of-survey-results-with-other-data-sourcesaggregate-statistics",
    "href": "source.html#comparison-of-survey-results-with-other-data-sourcesaggregate-statistics",
    "title": "Nonresponse Bias",
    "section": "Comparison of survey results with other data sources/aggregate statistics",
    "text": "Comparison of survey results with other data sources/aggregate statistics\nThe most frequently used method for studying nonresponse bias is to compare the survey data with information from another, more accurate, data source. One example of this is the comparison of survey results with data from Germany’s Microcensus, which has a low nonresponse rate because participation is compulsory (see, e.g., Koch, 1998). As a rule, such a comparison must be restricted to a few sociodemographic characteristics. The prerequisite for the comparison is that these characteristics were collected in a comparable way and that there are no (marked) differences between the target population and the time of data collection of the two surveys. In practice, the measurements are often not completely identical. Differences may then stem both from nonresponse error and from different measurements. Because both the survey in question and the Microcensus are based on samples, sampling error must also be taken into account in the comparison. Apart from that, it should be noted that it is not possible to infer from the absence of differences between the demographic characteristics in question that the actual survey variables of interest are unbiased. Bias, or the absence of bias, in demographic characteristics determines bias in other variables only to the extent that these variables are correlated with the respective demographic characteristics. In a meta-analysis, Groves & Peytcheva (2008) found that bias in substantive survey variables could not be predicted by bias in demographic characteristics."
  },
  {
    "objectID": "source.html#analysis-of-individual-data-for-respondents-and-nonrespondents",
    "href": "source.html#analysis-of-individual-data-for-respondents-and-nonrespondents",
    "title": "Nonresponse Bias",
    "section": "Analysis of (individual) data for respondents and nonrespondents",
    "text": "Analysis of (individual) data for respondents and nonrespondents\nAnother method for investigating nonresponse bias uses information that is available for the entire survey sample (respondents and nonrespondents). If such information is available, the data for the respondents can be compared to those for the nonrespondents in order to obtain estimates for the nonresponse bias. Relevant information may already be included in the sampling frame itself. Examples include information on age and gender in a population register sample or information on the duration of membership in the case of a survey of members of an association. Further information can be gained if individual data from other sources can be matched to the sample (for example, health or employment data from administrative records). Sometimes, aggregate information may also be used. The MICROM data (MICROM, 2011), for example, which were originally collected for the purpose of direct marketing, contain information for aggregates of eight households or for street sections. This information can be matched to a sample on the basis of household addresses (see Goebel et al., 2007). Observations by interviewers are a further source of information about the entire sample. In the European Social Survey, for example, interviewers are instructed to classify for all sampled cases the type of house in which the target person lives and to make and record certain observations about the immediate vicinity (Stoop et al., 2010).\nThe advantage of this method is that comparable measurements are then available for survey respondents and nonrespondents. On this basis, estimates of nonresponse bias can be gained for the observed (auxiliary) variables. In addition, profiles can be created for the different categories of nonresponse (e.g., target persons who could not be contacted or who were unwilling to cooperate) thereby providing clues about the sources of the nonresponse bias. However, the available characteristics do not usually include the actual survey variables of interest (if they did, the survey would not have been necessary). In general, it is important that the auxiliary variables used are closely correlated with the survey variables of interest (this can be empirically investigated for the group of survey respondents) and that further analyses reveal that they are also linked to participation behaviour. If this is the case, information about the presumed nonresponse bias of the survey variables of interest can be derived from the auxiliary variables. One potential disadvantage of this method is that the information in question is frequently not available for all sample units. Moreover, the quality of the measurement of the auxiliary variables is sometimes questionable. For example, observations frequently vary across interviewers (on the quality of interviewer observations, see, e.g., Olson, 2012)."
  },
  {
    "objectID": "source.html#analysis-of-variations-within-the-group-of-respondents",
    "href": "source.html#analysis-of-variations-within-the-group-of-respondents",
    "title": "Nonresponse Bias",
    "section": "Analysis of variations within the group of respondents",
    "text": "Analysis of variations within the group of respondents\nThe aim of this design is to gain information about nonresponse bias by comparing different subgroups of survey participants. To this end, respondents are divided into two groups – easy and difficult cases – according to the level of effort expended to obtain an interview. The division of the cases may be based on different criteria. For example, (a), early responders can be distinguished from those who were recruited later in the survey period. Or (b), respondents can be distinguished according to the number of contact attempts that were necessary. Also conceivable, (c), is a distinction according to whether respondents were immediately willing to participate or whether they initially refused and could be convinced to take part in the interview only after further attempts were made to encourage them to do so. These paradata are compiled during the fieldwork period – usually in contact protocols. The greatest advantage of this method is that it allows one to investigate for every survey variable whether there are differences between the different subgroups. The most serious disadvantage is that no information is available about the actual nonrespondents. The – usually untested – assumption is that the nonrespondents are more similar to the “difficult” cases than to the “easy” cases. Empirically, however, this is by no means inevitably the case, as demonstrated, for example, in a study conducted by Lin & Schaeffer (1995). A further limitation of this approach is that the indicators of difficulty cannot be unequivocally assigned to processes of reachability or willingness to be interviewed. It is advisable, for example, not to distinguish respondents according to the total number of contact attempts, but rather to consider only the contact attempts prior to the first contact, as this distinction is more directly related to the process of reachability.\nA special case of the analysis of variations within the group of survey respondents is to conduct a special nonresponse follow-up study. Such a study is usually carried out after the actual survey, and an attempt is made to persuade the nonrespondents (or a subset of the nonrespondents) to participate in the survey after all by increasing the effort expended (e.g., by offering monetary incentives). To increase the chances of success, the length of the questionnaire is sometimes reduced and/or a different survey mode is used (see Stoop et al., 2010). However, this renders it more difficult to compare the participants in the original survey to those in the follow-up study. The different field periods may also raise problems in this regard, especially when it cannot be ruled out that the variables measured change over time (as in the case of the measurement of attitudes, for example). However, the fundamental problem with nonresponse follow-up studies is that, despite all efforts, interviews cannot usually be conducted with all the target persons. In other words, the follow-up study also faces a (sometimes considerable) nonresponse problem."
  },
  {
    "objectID": "source.html#analysis-of-the-effects-of-different-weighting-procedures",
    "href": "source.html#analysis-of-the-effects-of-different-weighting-procedures",
    "title": "Nonresponse Bias",
    "section": "Analysis of the effects of different weighting procedures",
    "text": "Analysis of the effects of different weighting procedures\nFor the sake of completeness, it should be mentioned here that comparing unweighted (or merely design-weighted) data with nonresponse-weighted data may yield information about the extent to which nonresponse bias can be compensated for by weighting (see also the GESIS Survey Guidelines contribution “Weighting”; Gabler et al., 2016). If different weighting procedures are available, their results can be compared with each other and with the unweighted results. The main problem with this method is that no binding standard is available that might help one to decide which results reflect reality better."
  },
  {
    "objectID": "source.html#references",
    "href": "source.html#references",
    "title": "Nonresponse Bias",
    "section": "References",
    "text": "References\n\n\nBeullens, K., & Loosveldt, G. (2012). Should high response rates really be a primary objective? Survey Practice, 5(3), 1–5. https://doi.org/10.29115/sp-2012-0019\n\n\nBlohm, M., & Koch, A. (2009). Ausschöpfungsquoten und Stichprobenqualität am Beispiel des ALLBUS 2008: Führt eine höhere Ausschöpfung zu anderen/besseren Umfrageergebnissen? Quality of Large-Scale Surveys.\n\n\nBrick, J. M. (2013). Unit nonresponse and weighting adjustments: A critical review. Journal of Official Statistics, 29(3), 329–353. https://doi.org/10.2478/jos-2013-0026\n\n\nDixon, J., & Tucker, C. (2010). Survey nonresponse. In P. V. Marsden & J. D. Wright (Eds.), Handbook of survey research (2nd ed.) (pp. 593–630). Emerald.\n\n\nGabler, S., Kolb, J.-P., Sand, M., & Zins, S. (2016). Gewichtung (GESIS survey guidelines). GESIS - Leibniz Institute for the Social Sciences. https://doi.org/10.15465/GESIS-SG_EN_007\n\n\nGoebel, J., Spieß, C. K., Witte, N. R. J., & Gerstenberg, S. (2007). Die verknüpfung des SOEP mit MICROM-indikatoren: Der MICROM-SOEP datensatz. DIW Data Documentation 26. http://www.diw.de/documents/publikationen/73/diw_01.c.78103.de/diw_datadoc_2007-026.pdf\n\n\nGroves, R. M. (2006). Nonresponse rates and nonresponse bias in household surveys. Public Opinion Quarterly, 70(5), 646–675. https://doi.org/10.1093/poq/nfl033\n\n\nGroves, R. M., Fowler, F. J., Couper, M. P., Lepkowski, J. M., Singer, E., & Tourangeau, R. (2009). Survey methodology. Wiley. https://books.google.de/books?id=HXoSpXvo3s4C\n\n\nGroves, R. M., & Peytcheva, E. (2008). The impact of nonresponse rates on nonresponse bias: A meta-analysis. Public Opinion Quarterly, 72(2), 167–189. https://doi.org/10.1093/poq/nfn011\n\n\nGroves, R. M., Presser, S., & Dipko, S. (2004). The role of topic interest in survey participation decisions. Public Opinion Quarterly, 68(1), 2–31. https://doi.org/10.1093/poq/nfh002\n\n\nKeeter, S., Kennedy, C., Dimock, M., Best, J., & Craighill, P. (2006). Gauging the impact of growing nonresponse on estimates from a national RDD telephone survey. Public Opinion Quarterly, 70(5), 759–779. https://doi.org/10.1093/poq/nfl035\n\n\nKeeter, S., Miller, C., Kohut, A., Groves, R. M., & Presser, S. (2000). Consequences of reducing nonresponse in a national telephone survey. Public Opinion Quarterly, 64(2), 125–148. https://doi.org/10.1086/317759\n\n\nKoch, A. (1998). Wenn \"mehr\" nicht gleichbedeutend mit \"besser\" ist: Ausschöpfungsquoten und Stichprobenverzerrungen in allgemeinen Bevölkerungsumfragen. ZUMA Nachrichten, 22(42), 66–90.\n\n\nKoch, A., Fitzgerald, R., Stoop, I., Widdop, S., & Halbherr, V. (2012). Field procedures in the european social survey round 6: Enhancing response rates. http://www.europeansocialsurvey.org/docs/round6/methods/ESS6_response_enhancement_guidelines.pdf\n\n\nLin, I.-F., & Schaeffer, N. C. (1995). Using survey participants to estimate the impact of nonparticipation. Public Opinion Quarterly, 59(2), 236. https://doi.org/10.1086/269471\n\n\nMICROM. (2011). Microm datenhandbuch. Arbeitsunterlagen für microm MARKET & GEO.\n\n\nMontaquila, J. M., & Olson, K. M. (2012). Practical tools for nonresponse bias studies. http://www.amstat.org/sections/srms/webinarfiles/NRBiasWebinarApril2012.pdf\n\n\nOlson, K. (2012). Paradata for nonresponse adjustment. The ANNALS of the American Academy of Political and Social Science, 645(1), 142–170. https://doi.org/10.1177/0002716212459475\n\n\nPeytchev, A., Riley, S., Rosen, J., Murphy, J., & Lindblad, M. (2010). Reduction of nonresponse bias through case prioritization. Survey Research Methods, Vol 4, No 1 (2010). https://doi.org/10.18148/SRM/2010.V4I1.3037\n\n\nStoop, I., Billiet, J., Koch, A., & Fitzgerald, R. (2010). Improving survey response: Lessons learned from the european social survey. Wiley.\n\n\n\nFurther references:\nGroves, R. M., & Couper, M. P. (1998). Nonresponse in Household Interview Surveys. New York: John Wiley & Sons.\nSchnell, R. (1997). Nonresponse in Bevölkerungsumfragen. Ausmaß, Entwicklung und Ursachen. Opladen: Leske & Budrich. Retrieved from http://kops.ub.uni-konstanz.de/xmlui/bitstream/handle/urn:nbn:de:bsz:352-opus56148/Nonresponse_in_Bevoelkerungsumfragen.pdf?sequence=1\n\n\nJournal special issues on the subject of nonresponse\nThe Annals of the American Academy of Political and Social Science (2013), 645(1): The Nonresponse Challenge to Surveys and Statistics\nJournal of Official Statistics (2011), 27(2)\nJournal of the Royal Statistical Society: Series A (2013), 176(1): The use of paradata in social survey research\nPublic Opinion Quarterly (2006), 70(5): Special Issue: Nonresponse bias in household surveys"
  },
  {
    "objectID": "source2.html",
    "href": "source2.html",
    "title": "When are non-probability surveys fit for my purpose?",
    "section": "",
    "text": "In recent years, non-probability surveys have risen in popularity in empirical social science, which is not least due to the increasing use of the Internet as a means of conducting surveys. Although non-probability surveys are not tied to the online mode, it is particularly easy to quickly survey many people online. Despite their various limitations, non-probability (online) surveys are frequently used because of their low cost per interview and shorter field periods compared to most probability-based surveys. This guideline aims to provide an overview of the limitations associated with non-probability surveys and helps readers to make informed decisions on when and how to use them appropriately. We start by outlining fundamental principles of probability surveys, followed by a definition of non-probability surveys accompanied by examples. Next, we discuss various problems that are inherent to non-probability surveys. We then consider situations in which non-probability surveys might be fit for a specific purpose. Finally, we provide a summary as well as warnings for conducting or using non-probability surveys."
  },
  {
    "objectID": "source2.html#examples-of-non-probability-surveys",
    "href": "source2.html#examples-of-non-probability-surveys",
    "title": "When are non-probability surveys fit for my purpose?",
    "section": "2.1 Examples of non-probability surveys",
    "text": "2.1 Examples of non-probability surveys\nThe following section provides an overview of often-used types of non-probability surveys in social science research. The typology is mainly based on the AAPOR task-force report by Baker et al. (2013) on “Non-Probability Sampling.”\n\nVolunteer Sampling\nIn Volunteer Sampling, advertisements for the survey are fielded on web pages, in newspapers, or by using similar tools, and interested individuals select themselves into the set of respondents (see Baker et al., 2013; Statistics Canada, 2021, p. 34). Individuals who are interested in participating are invited to register on a survey’s website or directly to participate in a survey voluntarily.\nVolunteer Sampling can be conducted in any survey mode. It is frequently used to recruit members of opt-in or online access panels (see Callegaro et al., 2014; Statistics Canada, 2021, p. 35). Registered respondents' information is stored by the survey agency, and this list is used as some kind of replacement for a sampling frame to invite them to additional surveys in the future. However, this replacement of a sampling frame is based on an arbitrary selection and may differ systematically from the target population in many characteristics. A second application of Volunteer Sampling is crowd-sourcing (e.g., Amazon Mechanical Turk), where registered individuals are paid to complete different tasks, such as participating in a survey. Crowd-sourcing is different from an opt-in panel in that the volunteers are not mainly recruited on the platform to participate in surveys but to do any possible online tasks, one of which could be filling in a survey. Another practical example of volunteer samples is student samples, where surveys are conducted at universities with university students who are recruited online or in person. Volunteer sampling can also be conducted in malls or other public places where interviewers approach passers-by and ask them to participate in the survey.\n\n\nRiver Sampling\nRiver Sampling is a form of Volunteer Sampling that is exclusively conducted online. Advertisements are fielded during visits to websites or when conducting other online activities. For example, river samples may be advertised in apps or on social media. Baker et al. (2013, p. 18) describe river samples as opt-in web samples that intercept possible respondents online, doing other things (see also Baker et al., 2010, p. 14). Potential participants are often approached via banners or pop-ups to invite them to participate in the survey. Callegaro et al. (2014) differentiate river samples from access panels: In contrast to non-probability panels, the advertisement leads directly to a survey without any registration step. Initial screening questions can still be implemented to allow quota sampling. The next time the same respondents see a similar ad, it may lead them to a different survey.\nSocial media sampling is a common type of River Sampling. Previously gathered information on the users by the social media platform may be used to send survey advertisements specifically to target population members.1\n\n\nSnowball Sampling and Respondent Driven Sampling\nSnowball Sampling is another method to generate a non-probability survey that is particularly suitable to sample persons of hard-to-reach societal groups (e.g., Bacher et al., 2019; Lohr, 2022; Statistics Canada, 2021, p. 35), such as minorities or respondents with special health conditions. Here, an initial sample is collected using either non-probability or probability methods. In the following step, the respondents of the survey themselves hint at or invite additional respondents. Snowball Sampling is often used by asking respondents of hard-to-reach populations (e.g., refugees) to provide contact information of other members of the same group. One follows this approach several times until the desired number of respondents is reached. Respondent Driven Sampling is a method very similar to Snowball Sampling, with the difference that respondents (seeds) invite additional respondents with the help of coupons or special links. This allows researchers for every further respondent to know who invited them (Lee et al., 2017). When certain assumptions are met, it is possible that Respondent Driven Sampling can lead to a probability survey (Lee et al., 2017). However, one of these assumptions requires long enough chains of survey invitations, which are often not given due to selective nonresponse, leading to some chains ending far too early (Gile et al., 2015; Lee et al., 2017). Additionally, for both Snowball Sampling and Respondent Driven Sampling, it may make a difference whether they start using a probability or non-probability survey (Goodman, 2011), although, even when starting with a probability survey, it likely leads to a non-probability survey."
  },
  {
    "objectID": "source2.html#characteristics-of-probability-surveys",
    "href": "source2.html#characteristics-of-probability-surveys",
    "title": "When are non-probability surveys fit for my purpose?",
    "section": "2.2 Characteristics of probability surveys",
    "text": "2.2 Characteristics of probability surveys\nA probability survey is based on a random sample of units from the target population. The target population is defined as “the complete collection of observations we want to study” (Lohr, 2022, p. 4). This could be, for example, all German citizens living in private households that are older than 17 years. If the target population is large, surveying the entire target population would be unfeasible within a given time frame, too expensive or inefficient, so usually only a sample of the units is selected. A sample is a subset of the target population. The units of the sample are studied to infer the target population. In some cases, lists of units of the target population are available from which the sample can be drawn. These sampling frames might be lists of pupils in a school or the residents of a specific city from official registers. In other cases, sampling is based on a small list of randomly selected addresses, which are a starting point, to conduct random routes sampling (Bauer, 2016). Random routes selection can either be done by generating a list of addresses that is afterward handed over to interviewers to contact the residents, or by the interviewers in the same step as the contacting. The inclusion probability is a target population unit’s probability of being selected for a specific sample (e.g., Cochran, 1977). For a sample to be called a probability sample, units are selected randomly from the target population. As a crucial prerequisite for probability surveys, every unit in the target population must have a positive and known inclusion probability. If this is not fulfilled, inferences from the sample to the target population are not valid.\nIn the simplest form of a probability sample, a simple random sample, the inclusion probability is equal for every unit of the target population. Inclusion probabilities in complex random samples are non-zero and known but not equal, which must be accounted for by applying design weights to draw valid inferences. It is usually not possible to conduct an interview with every unit drawn so that the final sample of actual respondents (the net sample) differs from the sample originally drawn (the gross sample). Both can lead to errors in the inference from probability surveys. It is important to recognize possible errors in order to minimize errors when inferring from surveys to the population. The representation side of Groves' Total Survey Error framework [TSE; R. M. Groves & Lyberg (2010)] makes it possible to systematically consider errors in sampling and the realization of the net sample - i.e., the representation of the population. In the following section, we will go through the individual sources of error to discuss whether they are applicable to non-probability surveys later on.\nFirst, coverage error is the source of error that stems from differences between the population and the sample frame. For example, if a sample frame (from which the sample is drawn) does not cover the whole population, meaning that some members of the population have zero probability to be selected into the sample (undercoverage) this is a form of coverage error. The opposite can also be true if the sample frame covers persons that are not part of the target population (overcoverage).\nSecond, sampling error (R. Groves et al., 2009) describes errors that arise when the sample is drawn from the sampling frame. In some instances, the sample may differ from the target population by design. For example, it could be that one region is oversampled to guarantee a higher sample size needed for statistical analyses. If the sample is not a simple random sample, for example, because respondents have unequal probabilities of being selected into the sample, the statistical analysis needs to take that into account, for instance, by applying design weights.\nThe third error type of the TSE is called nonresponse error. Nonrespondents are sampled units who are eligible for participation but did not participate in the survey for any reason (e.g., non-contact, refusal). The amount of nonrespondents in relation to the number of eligible sample units is often quantified in terms of a response rate (RR2).\nAll of the types of errors can turn into bias, if they are systematic, meaning that some individuals have a higher propensity than others to be covered on the sampling frame, drawn for the sample, and/or to participate in a survey (R. M. Groves, 2006; e.g., Rubin, 1976). If the reasons for not responding to a survey are not associated with the variables of interest (i.e., the error is not systematic for the variable of interest), the separate cause model holds (the nonresponse models discussed in this section can be found in R. M. Groves, 2006), and no nonresponse bias is to be expected for this variable. However, when the same characteristics are associated with responding to a survey and the variable of interest, the common cause model holds. In this situation, nonresponse bias in the estimation of the variable of interest is very likely but may be reduced by adjustment weights that incorporate the characteristics that cause nonresponse and relate to the variable of interest. If the propensity to respond depends on the variable of interest itself, the survey variable cause model is true, which leads to nonresponse bias in estimation that can not be eliminated by any adjustment method.3\nTo address nonresponse bias in probability surveys, there are well-established adjustment weighting methods. One method would be to derive adjustment weights that make use of benchmark data, which is an essentially error-free external data source (e.g., Census data), and weight the survey respondents in a way that the distribution of survey respondents matches the distribution of the target population for all variables that are available from the benchmark data and the survey data. These variables are usually called auxiliary variables that are usually not equal to the variables of interest. Another method would be to use auxiliary variables that are available for the survey respondents and nonrespondents to model the nonresponse process and derive adjustment weights. Both methods need auxiliary variables that are highly correlated to the variables of interest and the nonresponse mechanism. To be effective, nonresponse weights ideally need to incorporate all variables that are related to both the nonresponse and the variable of interest. In addition, the nonresponse must not depend on the variable of interest itself.4"
  },
  {
    "objectID": "source2.html#characteristics-of-non-probability-surveys",
    "href": "source2.html#characteristics-of-non-probability-surveys",
    "title": "When are non-probability surveys fit for my purpose?",
    "section": "2.3 Characteristics of non-probability surveys",
    "text": "2.3 Characteristics of non-probability surveys\nThe most important difference between probability and non-probability surveys is that in the latter the requirements for inclusion probabilities are violated. In the absence of a proper sampling frame, the probability of inclusion is unknown for the members of the stated target population, and large parts of the target population have a zero probability of inclusion (Callegaro et al., 2015). If an online survey is, for example, advertised on a news website, it can not be known which members of the target population have a chance of seeing the advertisement and how high their propensity to see it is.\nIn non-probability surveys, the process of inclusion is unknown, as there is no sampling frame and thus no sampling stage. The inclusion is often highly (self-) selective and dependent on the characteristics of the participants, meaning that participants with some characteristics might be overrepresented in the survey, while others might be underrepresented or even completely missing. One example would be a survey of the German population, which is recruited only online, e.g., in an online access panel, underrepresenting respondents less familiar with the internet. Lacking a proper sampling process with a well-defined sampling frame, the error types of the TSE, such as coverage error, sampling error, and nonresponse error, cannot be distinguished, quantified, nor properly corrected by traditional adjustment procedures. Moreover, bias caused by the survey invitation process and self-selection, for example, getting access to the survey or refusing to participate in the survey, are impossible to disentangle. For instance, consider, a river sample recruited via Facebook advertisements. Population members who do not use Facebook can not participate in the survey, which can cause coverage bias if the target population is not defined as Facebook users. Facebook users who are shown the advertisement but decide not to participate might introduce selection bias. It is impossible to disentangle these biases because only information is available on survey participants and not on invited Facebook users. In addition, for many non-probability surveys, response rates are impossible to calculate. Even in cases in which the exact number of survey invitations as well as the number of refused invitations is known, for example, if members of an access panel are invited to participate in a specific survey, the unknowable selection probabilities in the panel recruitment steps make them an uninformative indicator of nonresponse (The American Association for Public Opinion Research, 2023, p. 78). The high amount of selectivity when recruiting for a non-probability survey makes it highly likely for survey variables to be systematically different from the population value due to (self-) selection bias. In addition, it is very unlikely to know in advance and observe all variables needed to correct for selection bias caused by potentially multiple and largely unknown mechanisms. The success of adjustment weights to correct for this potentially big amount of self-selection bias in the estimation is not guaranteed and is rather doubtful, as we will discuss in Section 3.1."
  },
  {
    "objectID": "source2.html#inference-from-non-probability-surveys-rely-on-too-strong-and-untestable-assumptions",
    "href": "source2.html#inference-from-non-probability-surveys-rely-on-too-strong-and-untestable-assumptions",
    "title": "When are non-probability surveys fit for my purpose?",
    "section": "3.1. Inference from non-probability surveys rely on (too) strong and untestable assumptions",
    "text": "3.1. Inference from non-probability surveys rely on (too) strong and untestable assumptions\nAs stated above, statistical inference from the sample to the target population is only valid if every respondent from the target population has a positive and known probability of being included in the sample. These requirements are not met for non-probability surveys. However, even for probability surveys, a random sample does not lead to a random sample of survey responents if the response rate is not 100% (e.g., Lohr, 2023, p. 5). Systematic nonresponse can lead to nonresponse bias. For probabilistic surveys, however, a broad strand exists in the literature on under which conditions valid inference is possible, even for surveys that suffer from nonresponse. In the following, we discuss two key assumptions made to draw valid inferences if not every sampled individual responds to the survey. These are: the assumptions of ignorability and positivity (e.g., A. W. Mercer et al., 2017; Rosenbaum & Rubin, 1983).\nIgnorability (A1) means in the survey setting that all mechanisms by which the respondents are selected for the survey are independent of the variables of interest, either unconditionally or conditional upon observed covariates that are associated with participation and the variable of interest, the so-called confounding variables. It is important to note that if only conditional ignorability can be assumed, the results must be weighted, using the confounders as auxiliary variables. Let us, for example, assume that our variable of interest (e.g., technical competence) dependends on Internet use, which also affects survey participation. Non-participation is only ignorable conditional on Internet use. When accounting for differences in Internet use between the survey respondents and the target population with the help of weighting, the ignorability assumption is fulfilled. However, if we did not measure Internet use, or if non-participation depends on other unknown or unmeasured characteristics that are not part of the weighting process, we will encounter non-ignorable bias in target population inference that can not be fully corrected.\nFor positivity (A2), all possible subgroups defined by confounding variables must be represented in the survey. Let us assume that technical competence is not only dependent on Internet use but also on age and that these characteristics also affect survey participation. Let us further assume that both confounders are measured in the survey. In such a case, adjustment weights can reduce bias in the variable of interest, e.g., technical competence, if we know the distribution of age and Internet use in the target population, for example, from a census (that has little or almost no error). If, however, there is, no Internet user of 70 years or older in the survey (but does exist in the target population) no weighting procedure can account for subgroups that are entirely missing from the survey.\nFor non-probability surveys, it is very likely that these assumptions are not fulfilled (Valliant & Dever, 2011, p. 134), but it is also impossible to examine whether they actually are fulfilled for any non-probability survey because there could always be an unknown uncontrolled confounder that might stem from self-selection or sample selection (Lohr, 2023, p. 8)."
  },
  {
    "objectID": "source2.html#representation-errors-can-not-be-differentiated",
    "href": "source2.html#representation-errors-can-not-be-differentiated",
    "title": "When are non-probability surveys fit for my purpose?",
    "section": "3.2. Representation errors can not be differentiated",
    "text": "3.2. Representation errors can not be differentiated\nFor non-probability surveys, as discussed in Section 2.2, different kinds of representation error (i.e., bias due to coverage error, sampling error, or nonresponse error) can, in general, not be distinguished. In the absence of a sampling frame, let alone a sample, coverage error or nonresponse error are not meaningful concepts. Therefore, many researchers use the term (self-)selection bias when discussing the representation error in non-probability surveys (Baker et al., 2013; Kohler et al., 2019). As the selection into the sample in non-probability surveys often depends mainly on self-selection (see, for example, Callegaro et al., 2015, p. 8) or arbitrary selection by the recruiter or interviewer, unequal selection probabilities are common but cannot be mitigated because they are unknown. This is also the case for respondents sampled randomly from a source that suffers from selection bias (e.g., Baker et al., 2010, p. 36). In many non-probability surveys, one source of selection bias lies in the decision to visit the place or website within the exact time frame when one advertises or conducts the survey. To take a survey sampled on a news website, for example, respondents who visit the news website daily for one hour might have a higher chance of seeing the advertisement and participating in the survey than respondents who visit the website only once a week. Similarly, respondents who do not have access to the Internet or the specific website, do not have a chance to participate in the survey.\nNonresponse is a form of reverse self-selection (Lehdonvirta et al., 2020, p. 139), caused, for example, by declined or unanswered survey invitations. However, in many non-probability surveys, for example using river sampling, invitations are not sent directly to individuals, so knowing who has seen the invitation (e.g., the ad in social media surveys) and did not participate is impossible (Baker et al., 2013). Even if it would be possible to determine who has seen an invitation, the characteristics that lead to ignoring a survey advertisement are likely different from those that lead to ignoring a direct survey invitation. In general, the participants can be expected to be a very small and rather selective group of those who could participate, potentially leading to larger and more complex biases than in probability surveys, where the potential number of participants is much smaller and limited to the sample (Valliant & Dever, 2011, p. 134). In addition to some potential respondents not answering the survey, the opposite problem can also exist in many non-probability surveys. As Lohr (2022) states, in a probability survey, target population members are (normally) only able to participate once and only if they are eligible respondents. However, in a non-probability survey, it is often possible to malevolently participate more than once or motivate others to participate and answer in a certain way to influence the results of the survey toward a specific outcome (see also Bethlehem, 2015), and those malevolent participants may be hard to identify. Regarding online surveys, it is even possible for malevolent actors to program bots to participate in the survey either with the intention of receiving the incentive or to influence survey results (Goodrich et al., 2023). With the recent technologies, those bots might be very hard or even impossible to differentiate from real respondents (Höhne et al., 2024).\nIn some non-probability surveys (e.g., online access panel or student samples), potential participants are asked directly to participate by mail or another method. Here, calculating a response rate may be possible, but we recommend avoiding the term response rate in the context of non-probability surveys and instead suggest different terms, such as a “participation rate” (see also, Baker et al., 2013; The American Association for Public Opinion Research, 2023). As mentioned above, online access panel members are usually not randomly selected in the first place, and the fact that the access panel is nonprobability-based should be reflected in a terminology that is different than the terminology for probability-based surveys."
  },
  {
    "objectID": "source2.html#weighting-is-likely-to-fail",
    "href": "source2.html#weighting-is-likely-to-fail",
    "title": "When are non-probability surveys fit for my purpose?",
    "section": "3.3. Weighting is likely to fail",
    "text": "3.3. Weighting is likely to fail\nIn probability surveys, design and adjustment weights are applied to account for unequal selection probabilities and systematic nonresponse. However, for adjustment weights to completely remove nonresponse bias, a number of requirements must be met, for example, that all auxiliary variables related to nonresponse and the variable of interest are included in the weighting process. This assumption is only testable if the true target population value (or at least a strong benchmark approximating the true target population value) for all variables of interest is available (Lohr, 2023, p. 8). In practical applications, it is very unlikely that the requirement is met. While weighting might still partially reduce bias, it is also possible that bias will increase (see, e.g., Yeager et al., 2011). Weighting is, however, more likely to be successful in probability surveys because, it only has to correct for bias due to nonresponse and capture characteristics related to the response decision, while the selection bias in nonprobability surveys might be affected by many decisions of potential participants, which each might be correlated with other characteristics. In other words, the ignorability (A1) assumption might be harder to fulfill in non-probability surveys because of the potentially higher number of mechanisms affecting survey participation. Additionally, the positivity assumption (A1) might also be harder to fulfill because efforts are made in probability surveys to target sampled individuals (Lohr, 2023, p. 6). Therefore, one can assume that missing population subgroups entirely is less likely in probability than in non-probability surveys.\nFor non-probability surveys, design weights cannot exist per definition. However, adjustment weights are sometimes proposed to mitigate all kinds of bias simultaneously (e.g., Baker et al., 2013, p. 70; Callegaro et al., 2015, p. 183). This means that the auxiliary variables need not only to be correlated to the nonresponse mechanisms but also to an unknown number of other selection mechanisms, making the ignorability assumption (A1) even more complicated to fulfill. At present, there is little reliable knowledge about whether and which weighting methods generally work for non-probability surveys (Cornesse & Blom, 2020, p. 21). In general, there is large evidence that the success of weighting is far less affected by the specific weighting method than by the variables included in the weighting process (see, for example, A. Mercer et al., 2018). More research is needed in this area. This is in contrast to probability surveys, where weighting is well-established and based on a large body of literature (e.g., Edelman, 2023)."
  },
  {
    "objectID": "source2.html#quotas-are-no-solution",
    "href": "source2.html#quotas-are-no-solution",
    "title": "When are non-probability surveys fit for my purpose?",
    "section": "3.4. Quotas are no solution",
    "text": "3.4. Quotas are no solution\nA common attempt to introduce control into the sample selection of non-probability surveys is to apply quota sampling. In quota sampling, reference data are used to get information on the composition of the target population regarding selected quota variables, and the survey is conducted in a way that the composition of survey respondents regarding the quota variables matches their composition in the target population. The most popular quota variables are demographic variables, but some other variables might also be used, for example, the proportion of persons in the target population with Internet access. Then, the sample selection process aims to recruit a sample that matches those quotas. Quotas can be as simple as one or more univariate distributions but also cross-tabulated distributions (e.g., by age and gender).\nIn opt-in panels, quota sampling can be done by using known information on the panel members to only invite respondents according to those quotas. If we know, for example, that the target population consists of 55% female and 45% male persons a quota would demand that 55% of respondents are women.\nOnly when the quota variables are (highly) correlated with the selection mechanisms and the variables of interest, the accuracy of estimates for the variables of interest can be improved as well. Quotas implicitly assume that all characteristics that affect participation are captured in the quota. This assumption is very unrealistic, and it is nearly impossible to implement quotas for all needed variables, as it is impossible to know exactly what characteristics should be included. Only if all selection mechanisms are addressed in a quota (fulfilling the ignorability assumption), and the quota can be fulfilled (i.e., fulfilling the positivity assumption) accurate survey results can be achieved. One should be aware of these limitations when assessing results from surveys that are, for example, advertised as being representative of age, gender, and education."
  },
  {
    "objectID": "source2.html#a-higher-number-of-respondents-does-not-guarantee-lower-bias",
    "href": "source2.html#a-higher-number-of-respondents-does-not-guarantee-lower-bias",
    "title": "When are non-probability surveys fit for my purpose?",
    "section": "3.5. A higher number of respondents does not guarantee lower bias",
    "text": "3.5. A higher number of respondents does not guarantee lower bias\nProponents of non-probability surveys might argue that the possibility of reaching large sample sizes very quickly can make up for (some) problems with non-probability surveys, which is, however, not true (Meng, 2018). This misconception might stem from the fact that for probability surveys, larger sample sizes improve the precision of estimates. They also increase the ability to draw inferences even for subgroups that are rare in the target population.\nThe accuracy of an estimate depends on two factors: precision and bias. A typical measure of precision in a probability survey is the standard deviation, and a typical measure for the bias is the difference (e.g., in mean) of the survey estimate from the target population value. The Mean Squared Error (MSE) of an estimate, a common measure of accuracy, includes bias and precision (e.g., Kohler, 2019, Equation 5 on p. 153). The precision of a survey estimate is related to the sample size, that is, for a given estimate, the precision increases as the sample size increases. When the sample size is large, precision increases and the influence of a small number of outliers will be reduced.\nWhile larger sample sizes increase precision, there is no guarantee that they reduce bias. If, for example, an estimate has a high precision due to a large sample size but also a large bias, that survey will only allow to precisely measure biased parameters(s). This is also true for a non-probability survey that covers especially large proportions of the target population (e.g., Meng, 2018). Meng (2018) argues that the required sample size for a non-probability sample to reach a similarly low MSE as a small probability survey mainly depends on the correlation of the variable of interest with the probability of being part of the sample. Even a very small correlation (e.g., 0.05) can be a huge problem, leading to the need for a tremendous-sized set of respondents (i.e., more than half of the target population size) to produce results as accurate as a much smaller simple random survey (i.e., 400 respondents, independent of target population size)."
  },
  {
    "objectID": "source2.html#estimates-of-precision-need-a-lot-of-assumptions",
    "href": "source2.html#estimates-of-precision-need-a-lot-of-assumptions",
    "title": "When are non-probability surveys fit for my purpose?",
    "section": "3.6. Estimates of precision need a lot of assumptions",
    "text": "3.6. Estimates of precision need a lot of assumptions\nThere is no statistical formula that allows to estimate precision for non-probability surveys. Variance formulas and the like that are derived for probability sampling cannot simply be transferred to non-probability surveys. There are, however, some commonly used methods to cope with this problem. Examples include using jackknife or bootstrap procedures to estimate precision (e.g., Elliott & Valliant, 2017).\nIn jackknife variance estimation, the variance is calculated in a repeated procedure. One case is removed from the sample for the first step, and the estimate (e.g., mean, median, etc.) is calculated. Then, the same step is repeated by leaving the second case out of the original sample, and so forth. This procedure is repeated for every case in the survey. Finally, the gathered list of estimates is used to calculate the variance or confidence interval (e.g., Quenouille, 1956). Using the bootstrap method (Efron, 1979), repeated samples (around 2,000-10,000) are drawn from the original sample with replacement. Then again, the estimate is calculated for every bootstrap sample, and the measure of uncertainty is calculated from these estimates. Later, a Bayesian bootstrap was presented as an alternative to the standard bootstrap method (e.g., Rubin, 1981).\nBoth resampling methods were above described only in their most simple form, which assumes simple random sampling. In practical applications, every step in the sampling design, including quotas and all weighting steps, must be accounted for in every replication (McPhee et al., 2022). Such methods are commonly used to estimate the precision of estimates from probability surveys. For nonprobability surveys that lack a proper sampling design and thus design weights, and as their adjustment weights may not include all needed auxiliary variables, they are “likely to be too narrow.” (McPhee et al., 2022).5"
  },
  {
    "objectID": "source2.html#conclusion-on-the-generalizability-of-results",
    "href": "source2.html#conclusion-on-the-generalizability-of-results",
    "title": "When are non-probability surveys fit for my purpose?",
    "section": "3.7. Conclusion on the generalizability of results",
    "text": "3.7. Conclusion on the generalizability of results\nAs we discussed in the previous sections, there are many potential sources of bias in non-probability surveys. Also, it is rarely possible to detect these biases or determine their extent. In contrast to probability surveys, no well-established theory, like sampling theory, allows for statistical inference (e.g., Callegaro et al., 2015, pp. 52–56; Vehovar et al., 2016, p. 332). As we can never be sure whether the assumptions needed for an estimate to be accurate are fulfilled (A.1, A.2), the estimates of the survey may or may not be extremely biased, without us being able to know. This is even the case when quotas or weights are used. Under these conditions, it is impossible to generalize the results from a non-probability survey without an unreasonable risk of bias.\nIn contrast to generalizing the survey to the target population (e.g., expecting external validity), it can be less problematic to expect internal validity when using an experimental design within the survey (e.g., Baker et al., 2013, p. 41; Kohler, 2019; A. W. Mercer et al., 2017, p. 254). Using non-probability surveys for experiments will be discussed in section 4.1."
  },
  {
    "objectID": "source2.html#footnotes",
    "href": "source2.html#footnotes",
    "title": "When are non-probability surveys fit for my purpose?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee the survey guideline by Pötzschke et al. (2023) for more detailed information on social media sampling.↩︎\nSee the survey guideline by Stadtmüller et al. (2019) for more details on response rate calculation↩︎\nMore detailed information about the concept and analysis of nonresponse bias can be found in the survey guidelines by Koch & Blohm (2015) and Felderer (2024).↩︎\nMore information on weighting can be found in the survey guidelines by Gabler et al. (2016) and Sand (2020).↩︎\nMore information and recommendations on transparent reporting of precision in non-probability surveys can be found in the AAPOR Task Force Report on “Data Quality Metrics for Online Samples” by McPhee et al. (2022).↩︎"
  }
]